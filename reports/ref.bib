@article{falcon2019pytorch,
  title={Pytorch lightning},
  author={Falcon, William and others},
  journal={GitHub. Note: https://github. com/PyTorchLightning/pytorch-lightning},
  volume={3},
  pages={6},
  year={2019}
}



@article{ke2017lightgbm,
  title={Lightgbm: A highly efficient gradient boosting decision tree},
  author={Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={30},
  pages={3146--3154},
  year={2017}
}
@article{XGBoost2016,
   title={XGBoost},
   url={http://dx.doi.org/10.1145/2939672.2939785},
   DOI={10.1145/2939672.2939785},
   journal={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   publisher={ACM},
   author={Chen, Tianqi and Guestrin, Carlos},
   year={2016},
   month={Aug}
}
@misc{dorogush2018catboost,
      title={CatBoost: gradient boosting with categorical features support}, 
      author={Anna Veronika Dorogush and Vasily Ershov and Andrey Gulin},
      year={2018},
      eprint={1810.11363},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ruggles_ipums_2021,
	title = {{IPUMS} {USA}: {Version} 11.0 [dataset]},
	url = {https://doi.org/10.18128/D010.V11.0},
	publisher = {Minneapolis, MN: IPUMS},
	author = {Ruggles, Steven and Flood, Sarah and Foster, Sophia and Goeken, Ronald and Pacas, Jose and Schouweiler, Megan and Sobek, Matthew},
	year = {2021},
}

@inproceedings{narang_transformer_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Do {Transformer} {Modifications} {Transfer} {Across} {Implementations} and {Applications}?},
	url = {https://aclanthology.org/2021.emnlp-main.465},
	doi = {10.18653/v1/2021.emnlp-main.465},
	language = {en},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Narang, Sharan and Chung, Hyung Won and Tay, Yi and Fedus, Liam and Fevry, Thibault and Matena, Michael and Malkan, Karishma and Fiedel, Noah and Shazeer, Noam and Lan, Zhenzhong and Zhou, Yanqi and Li, Wei and Ding, Nan and Marcus, Jake and Roberts, Adam and Raffel, Colin},
	year = {2021},
	pages = {5758--5773},
}

@inproceedings{huang_improving_2020,
	title = {Improving {Transformer} {Optimization} {Through} {Better} {Initialization}},
	url = {https://proceedings.mlr.press/v119/huang20f.html},
	abstract = {The Transformer architecture has achieved considerable success recently; the key component of the Transformer is the attention layer that enables the model to focus on important regions within an input sequence. Gradient optimization with attention layers can be notoriously difficult requiring tricks such as learning rate warmup to prevent divergence. As Transformer models are becoming larger and more expensive to train, recent research has focused on understanding and improving optimization in these architectures. In this work our contributions are two-fold: we first investigate and empirically validate the source of optimization problems in the encoder-decoder Transformer architecture; we then propose a new weight initialization scheme with theoretical justification, that enables training without warmup or layer normalization. Empirical results on public machine translation benchmarks show that our approach achieves leading accuracy, allowing to train deep Transformer models with 200 layers in both encoder and decoder (over 1000 attention/MLP blocks) without difficulty. Code for this work is available here: {\textbackslash}url\{https://github.com/layer6ai-labs/T-Fixup\}.},
	language = {en},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Huang, Xiao Shi and Perez, Felipe and Ba, Jimmy and Volkovs, Maksims},
	month = nov,
	year = {2020},
	pages = {4475--4483},
}

@article{erickson_autogluon-tabular:_2020,
	title = {{AutoGluon}-{Tabular}: {Robust} and {Accurate} {AutoML} for {Structured} {Data}},
	shorttitle = {{AutoGluon}-{Tabular}},
	url = {http://arxiv.org/abs/2003.06505},
	abstract = {We introduce AutoGluon-Tabular, an open-source AutoML framework that requires only a single line of Python to train highly accurate machine learning models on an unprocessed tabular dataset such as a CSV file. Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection, AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in multiple layers. Experiments reveal that our multi-layer combination of many models offers better use of allocated training time than seeking out the best. A second contribution is an extensive evaluation of public and commercial AutoML platforms including TPOT, H2O, AutoWEKA, auto-sklearn, AutoGluon, and Google AutoML Tables. Tests on a suite of 50 classification and regression tasks from Kaggle and the OpenML AutoML Benchmark reveal that AutoGluon is faster, more robust, and much more accurate. We find that AutoGluon often even outperforms the best-in-hindsight combination of all of its competitors. In two popular Kaggle competitions, AutoGluon beat 99\% of the participating data scientists after merely 4h of training on the raw data.},
	journal = {arXiv:2003.06505 [cs, stat]},
	author = {Erickson, Nick and Mueller, Jonas and Shirkov, Alexander and Zhang, Hang and Larroy, Pedro and Li, Mu and Smola, Alexander},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.06505},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@article{paszke_pytorch:_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {http://arxiv.org/abs/1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	journal = {arXiv:1912.01703 [cs, stat]},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.01703},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Statistics - Machine Learning},
}
@article{narang2021transformer,
      title={Do Transformer Modifications Transfer Across Implementations and Applications?}, 
      author={Sharan Narang and Hyung Won Chung and Yi Tay and William Fedus and Thibault Fevry and Michael Matena and Karishma Malkan and Noah Fiedel and Noam Shazeer and Zhenzhong Lan and Yanqi Zhou and Wei Li and Nan Ding and Jake Marcus and Adam Roberts and Colin Raffel},
      year={2021},
      eprint={2102.11972},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{tan_efficientnetv2:_2021,
	title = {{EfficientNetV2}: {Smaller} {Models} and {Faster} {Training}},
	shorttitle = {{EfficientNetV2}},
	url = {http://arxiv.org/abs/2104.00298},
	abstract = {This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop this family of models, we use a combination of training-aware neural architecture search and scaling, to jointly optimize training speed and parameter efficiency. The models were searched from the search space enriched with new ops such as Fused-MBConv. Our experiments show that EfficientNetV2 models train much faster than state-of-the-art models while being up to 6.8x smaller. Our training can be further sped up by progressively increasing the image size during training, but it often causes a drop in accuracy. To compensate for this accuracy drop, we propose to adaptively adjust regularization (e.g., dropout and data augmentation) as well, such that we can achieve both fast training and good accuracy. With progressive learning, our EfficientNetV2 significantly outperforms previous models on ImageNet and CIFAR/Cars/Flowers datasets. By pretraining on the same ImageNet21k, our EfficientNetV2 achieves 87.3\% top-1 accuracy on ImageNet ILSVRC2012, outperforming the recent ViT by 2.0\% accuracy while training 5x-11x faster using the same computing resources. Code will be available at https://github.com/google/automl/tree/master/efficientnetv2.},
	journal = {arXiv:2104.00298 [cs]},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = jun,
	year = {2021},
	note = {arXiv: 2104.00298},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
